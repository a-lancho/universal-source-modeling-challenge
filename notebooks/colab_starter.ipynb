{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Colab Starter: Universal Source Modeling Challenge\n",
        "\n",
        "This notebook is a practical starting point for students to train at home and rehearse live-day evaluation.\n",
        "\n",
        "Key reminders:\n",
        "- Prediction is **sequential/online** (no lookahead).\n",
        "- The evaluator enforces a **context limit** and **runtime limit**.\n",
        "- Your competition-day submission is a Python file that defines `build_predictor(...)`.\n",
        "\n",
        "See also:\n",
        "- `COMPETITION_RULES.md`\n",
        "- `submissions/README.md`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import platform\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "repo_markers = [Path('competition'), Path('src'), Path('submissions')]\n",
        "in_repo_root = all(p.exists() for p in repo_markers)\n",
        "print('cwd:', Path.cwd())\n",
        "print('repo_root_detected:', in_repo_root)\n",
        "if not in_repo_root:\n",
        "    print('If this is Colab, clone/mount the repo and set the notebook cwd to the repository root.')\n",
        "    print('Expected markers:', [str(p) for p in repo_markers])\n",
        "\n",
        "# Optional (uncomment if needed):\n",
        "# import subprocess\n",
        "# subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'], check=True)\n",
        "\n",
        "print('python_version:', sys.version.split()[0])\n",
        "print('platform:', platform.platform())\n",
        "print('numpy_version:', np.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Availability\n",
        "\n",
        "Expected local paths for baseline practice:\n",
        "- Training data: `data/generator/train.npy`\n",
        "- Practice test data: `data/generator/test.npy`\n",
        "\n",
        "If these files are missing, ask the instructor for the practice split or generate one with the provided synthetic source tools (instructor-side workflow).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "train_path = Path('data/generator/train.npy')\n",
        "test_path = Path('data/generator/test.npy')\n",
        "\n",
        "missing = [str(p) for p in (train_path, test_path) if not p.exists()]\n",
        "if missing:\n",
        "    print('Missing required files for practice:', missing)\n",
        "    print('Place practice data under data/generator/ or ask the instructor for a practice split.')\n",
        "else:\n",
        "    train = np.load(train_path)\n",
        "    test = np.load(test_path)\n",
        "    print('train shape:', train.shape, 'dtype:', train.dtype)\n",
        "    print('test shape: ', test.shape, 'dtype:', test.dtype)\n",
        "    if train.ndim == 1 and train.size > 0:\n",
        "        print('train min/max symbol:', int(train.min()), int(train.max()))\n",
        "    if test.ndim == 1 and test.size > 0:\n",
        "        print('test min/max symbol: ', int(test.min()), int(test.max()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predictor Template\n",
        "\n",
        "Your submission file must define:\n",
        "\n",
        "```python\n",
        "def build_predictor(alphabet_size: int, max_context_length: int) -> Predictor:\n",
        "    ...\n",
        "```\n",
        "\n",
        "Start by copying or modifying `submissions/template_predictor.py`. The live evaluator imports this function dynamically.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Smoke Test (5000 tokens)\n",
        "\n",
        "This quickly verifies that your predictor imports correctly and produces the canonical `FINAL_SCORE` line. Expected `evaluated_tokens=5000`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    '-m', 'competition.run_live_eval',\n",
        "    '--test-path', 'data/generator/test.npy',\n",
        "    '--predictor-path', 'submissions/template_predictor.py',\n",
        "    '--smoke-test',\n",
        "]\n",
        "print('Running:', ' '.join(cmd))\n",
        "result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "print('returncode:', result.returncode)\n",
        "if result.stdout:\n",
        "    print(result.stdout.strip())\n",
        "if result.stderr:\n",
        "    print('STDERR:\n' + result.stderr.strip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Full Practice Evaluation (default 200000-token prefix)\n",
        "\n",
        "This mirrors the competition default behavior. It should print one canonical `FINAL_SCORE` line.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    '-m', 'competition.run_live_eval',\n",
        "    '--test-path', 'data/generator/test.npy',\n",
        "    '--predictor-path', 'submissions/template_predictor.py',\n",
        "]\n",
        "print('Running:', ' '.join(cmd))\n",
        "result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "print('returncode:', result.returncode)\n",
        "if result.stdout:\n",
        "    print(result.stdout.strip())\n",
        "if result.stderr:\n",
        "    print('STDERR:\n' + result.stderr.strip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tips / Extensions\n",
        "\n",
        "Common optimization knobs:\n",
        "- Keep `--validate-probabilities` off during normal runs (it is slower).\n",
        "- Avoid slow Python work inside `predict_next` (especially repeated allocations).\n",
        "- Cache expensive preprocessing/model state where possible.\n",
        "- Keep memory usage below the Colab Pro+ budget target (assume ~16GB GPU limit for competition design).\n",
        "\n",
        "Workflow reminders:\n",
        "- Training at home is allowed. Live day is inference/evaluation under time constraints.\n",
        "- Test both `--smoke-test` and a full practice run before competition day.\n",
        "- Do not modify the evaluator script or rely on lookahead.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}