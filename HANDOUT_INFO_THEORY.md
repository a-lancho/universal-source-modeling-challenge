# Information Theory Handout: Universal Source Modeling Challenge

This handout explains the competition in information-theoretic terms and gives practical guidance for baseline performance and workflow.

Notation follows the course conventions (random variables uppercase, realizations lowercase, alphabets calligraphic; logs base 2). See `docs/notation.md` for the shared score definition used across the repo.

## 1) What Is the Task?

You are given a sequence of symbols from a fixed finite alphabet `\mathcal{X}` (default size `A=|\mathcal{X}|=16`), generated by a source from the course source family (practice data uses an HMM-based generator).

Let `X_1^N` denote the random evaluated sequence and `x_1^N` the realized sequence. Your job is to build a **sequential predictor** that outputs conditional PMFs

`q_i(\cdot \mid x_1^{i-1})`, for `i=1,\dots,N`.

- At step `i`, predict `q_i(\cdot | x_1^{i-1})`
- You may use only past symbols (no lookahead)
- The evaluator reveals symbols one-by-one and scores your predictions online

Conceptually, this is a universal prediction / source modeling problem under computational constraints.

## 2) Score = Empirical Average Log-Loss (Coding Interpretation)

The competition score is empirical average log-loss / empirical cross-entropy (all logs base 2):

`\widehat{H}_Q(x_1^N) \triangleq (1/N) * sum_{i=1}^N -log2 q_i(x_i | x_1^{i-1})`

- Units: **bits/symbol**
- Lower is better
- This is the leaderboard score

Interpretation:
- `-log2 q_i(x_i | x_1^{i-1})` is the idealized codelength contribution (in bits) for symbol `x_i` under predictor `Q`
- The average is an empirical codelength-per-symbol proxy / average log-loss

LLMZip connection (same core quantity, different normalization):
- LLMZip’s core quantity is average token log-loss (bits/token), using the same `-\log_2 q_i(x_i)` terms and averaging over tokens
- It can then be normalized to bits/character via average (expected) characters per token
- Our competition uses a fixed finite alphabet and reports **bits/symbol**
- LLMZip discusses arithmetic coding as the near-optimal way to realize this codelength in a compressor
- We do **not** implement arithmetic coding in scoring; the evaluator scores the log-loss directly, so the leaderboard score is not an achieved compressed file size

## 3) Entropy, Cross-Entropy, KL, and Entropy-Rate Interpretation

Distributional definitions (base 2) for PMFs `P,Q` on a common alphabet:

- Entropy: `H(P) = -E_P[log2 P(X)]`
- Cross-entropy: `H(P,Q) = -E_P[log2 Q(X)]`
- KL divergence: `D(P||Q) = E_P[log2 (P(X)/Q(X))]`
- Identity: `H(P,Q) = H(P) + D(P||Q)`

Under stationarity/ergodicity, the evaluated average log-loss `\widehat{H}_Q(x_1^N)` can be interpreted as an empirical estimate of a cross-entropy-rate-like quantity.
At finite `N`, expect sampling noise and dataset-specific variation.

Conceptually, if your predictor matches the source conditionals well, the score approaches the source entropy rate. Excess score reflects modeling mismatch (redundancy).

Why higher-order n-grams can get worse:
- As order increases, contexts become sparse
- High-order estimates can overfit observed counts
- This is a bias-variance tradeoff / sparsity problem

So “larger context” is not automatically better unless smoothing/backoff/interpolation is handled carefully.

## 4) Constraints and Why They Matter (IT Framing)

This competition is about **prediction under constraints**, not just best possible log-loss in unlimited compute.

- **Context window limit** (`max_context_length = 256`)
  - Models are effectively finite-memory at evaluation time
  - This mirrors practical source coding/prediction settings

- **Time limit** (`time_limit_seconds = 600`)
  - Feasibility matters
  - A predictor that is accurate but too slow is not operationally valid

- **Fixed evaluation prefix** (`N = 200000`)
  - Everyone is scored on the same prefix length `N`
  - Improves fairness and runtime predictability

- **Timeout disqualification**
  - Timed-out runs are operationally invalid for ranking

## 5) Baselines and What “Good” Looks Like

Typical baseline behavior on our **public practice set** (empirical values; not theoretical constants; they vary with seed and dataset realization):

| Baseline | Typical bits/symbol |
|---|---:|
| Uniform (`A=16`) | `~4.0` |
| Hard-backoff n-gram (`n=3`, Laplace=1.0) | `~3.0–3.1` |
| Hard-backoff n-gram (`n=4`, Laplace=1.0) | `~3.0` (official simple baseline) |
| Threshold backoff n-gram (`n=5`, min_count=8`) | often competitive / more stable than hard `n=5` |
| Hard-backoff n-gram (`n=5`, Laplace=1.0) | can degrade due to sparsity |

Takeaway:
- Beating `~3.0` bits/symbol on the practice source family is already meaningful.
- A slower model that times out is not a valid improvement.

## 6) Encouraged Approaches (Mapped to IT Ideas)

### Mixtures / interpolation (universal prediction)

Combine multiple predictors or context orders instead of committing to one order. This is a classical route toward more robust universal prediction.

### Online updating vs pre-fitting

- **Pre-fitting** uses the provided training sequence
- **Online updating** adapts during test evaluation (still legal, since only past symbols are used)

Both are valid and often complementary.

### Smoothing and backoff strategies

Examples:
- Laplace smoothing (simple baseline)
- Threshold backoff (reduce sparse-context overfitting)
- Interpolated n-grams
- Kneser-Ney (conceptually relevant, though not required)

The central goal is good probability estimation under sparsity.

### Optional compression sanity check

The provided `zlib` / `lzma` / `bz2` sanity check is optional. It helps illustrate:

- Better prediction generally implies shorter idealized codelengths (and, with a matched arithmetic coder, shorter achievable codes in principle)
- Generic compressors may miss source structure your predictor captures

This is educational only and does not affect ranking.

### Bonus: arithmetic coding validation (practice set only)

Optional course bonus work (not leaderboard scoring): on the **public practice set only**, use a short fixed prefix (recommended `N_AC=30000`) and validate the coding interpretation with an arithmetic coder driven by your sequential PMFs `q_i(\cdot \mid x_1^{i-1})`.

- Compute `\widehat{H}_Q = (1/N_AC)\sum_{i=1}^{N_AC} -\log_2 q_i(x_i \mid x_1^{i-1})` (bits/symbol).
- Arithmetic coding should realize a compressed length close to `\sum_i -\log_2 q_i(x_i \mid x_1^{i-1})` when the coder is matched to your probabilities.
- Report the realized rate `bps_AC = compressed_bits / N_AC` and the gap `Delta_AC = bps_AC - \widehat{H}_Q`.
- Expect a small positive overhead from finite precision, renormalization, and stream termination (so `Delta_AC` is usually not exactly zero).
- The validation is only meaningful if decoding is lossless: decoded output must exactly equal `x_1^{N_AC}`.
- Do **not** run this on the live-day secret test; this is a practice-set-only course bonus.

## 7) How to Run (Student Commands)

Smoke test (5000 symbols):

```bash
python -m competition.run_live_eval \
  --test-path data/public_practice/test.npy \
  --predictor-path submissions/template_predictor.py \
  --smoke-test
```

Full practice evaluation (default `N=200000`-symbol prefix):

```bash
python -m competition.run_live_eval \
  --test-path data/public_practice/test.npy \
  --predictor-path submissions/template_predictor.py
```

Live day evaluation (template):

```bash
python -m competition.run_live_eval \
  --test-path data/live_release/test.npy \
  --predictor-path <YOUR_PREDICTOR.py>
```

Submission formatting:
- Submit the final `FINAL_SCORE ...` line exactly as printed
- You may prepend a label when reporting to the instructor:
  - `TEAM=<name> FINAL_SCORE ...`

## FAQ (Short)

### What is “lookahead”?

Using future test symbols (directly or indirectly) when predicting the current symbol. This is not allowed.

### Why is lookahead disallowed?

Because the task is sequential prediction / source modeling. Using future symbols breaks the coding interpretation and invalidates the score.

### What happens if my run times out?

On live day, timed-out runs are disqualified from ranking (see `COMPETITION_RULES.md`).

### Why only the first 200000 symbols?

To keep runtime predictable and ensure everyone is evaluated on the same prefix under the same constraints.
